# MLOps Taller 5 - Locust con MLflow #

**Grupo compuesto por Sebastian Rodr√≠guez y David C√≥rdova**

Este proyecto implementa una metodolog√≠a de pruebas de estres a una API que toma un modelo desde MLflow.

## Caracter√≠sticas Principales

- **Arquitectura  MLOps**: Despliegue de MLflow mediante contenedores.
- **Pipeline completo de ML**: Desde ingesta de datos hasta inferencia en producci√≥n con trazabilidad completa
- **Storage multi-capa especializado**:
  - Postgres para metadata de experimentos y modelos MLFlow
  - MinIO como S3-compatible para artefactos (modelos, plots, logs)
- **Contenerizaci√≥n orquestada**: Docker Compose gestiona toda la infraestructura de servicios
- **API de producci√≥n**: FastAPI consume modelos directamente desde MLFlow Registry
- **Tracking y versionado autom√°tico**: Experimentos, m√©tricas y modelos registrados autom√°ticamente
- **Configuraci√≥n S3 local**: MinIO simula AWS S3 para desarrollo y testing
- **Locust**: App utilizada para generar tr√°fico en la API y validad capacidad de respuesta 

## Estructura del Proyecto

```
fastapi/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ main.py
‚îî‚îÄ‚îÄ requirements.txt

locust/
‚îú‚îÄ‚îÄ locustfile.py
‚îî‚îÄ‚îÄ requirements-locust.txt

minio/

ml-training/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ train_model.py

mlflow/

register_model/
‚îú‚îÄ‚îÄ RegresionLogistica.pkl
‚îî‚îÄ‚îÄ wait_and_run.sh

docker-compose-locust.yaml
docker-compose.yaml
RegresionLogistica.pkl
requirements.txt
```

### Descripci√≥n de Componentes


**Estructura de servicios:**

- **fastapi/**:
  - **main.py**: API REST que consume modelos registrados en MLFlow para inferencia
  - **Dockerfile**: Containerizaci√≥n con dependencias ML y conexi√≥n a MLFlow
  - **requirements.txt**: Librer√≠as espec√≠ficas para servicio de predicciones

- **minio/**:
  - **Funci√≥n**: Volume mount para almacenamiento persistente de artefactos MLFlow
  - **Contenido**: Modelos serializados, plots, logs y metadata de experimentos
  - **Acceso**: S3-compatible storage accesible desde Jupyter y MLFlow server

- **images/**:
  - **Prop√≥sito**: Documentaci√≥n visual del proyecto
  - **Contenido**: Screenshots de interfaces, evidencias de funcionamiento
  - **Uso**: Soporte para README y documentaci√≥n t√©cnica
- **locust/**:
  - **funci√≥n**: alojar el .py que despliega la estructura de inferencia de locust 
  **requirements**: librer√≠as necesarias para desplegar locust
  - **Uso**: Soporte para README y documentaci√≥n t√©cnica

**Configuraci√≥n de orquestaci√≥n:**

- **docker-compose.yaml**:
  - **Servicios gestionados**: MinIO, Postgres, FastAPI, Mlflow
  - **Networking**: Red interna para comunicaci√≥n inter-servicios
  - **Vol√∫menes persistentes**: postgres_data para persistencia
  - **Variables de entorno**: Configuraci√≥n S3, credenciales y URIs de conexi√≥n
  - **Dependencias**: Orden de inicio optimizado para disponibilidad de servicios

  - **docker-compose-locust.yaml**:
  - **Servicios gestionados**:locust
  - **Dependencias**: FastAPI

**Servicios containerizados:**

- **MinIO Container**: S3-compatible storage (puertos 9000/9001)
- **Postgres Container**: Backend store MLFlow metadata (puerto 5432)
- **FastAPI Container**: API producci√≥n conectada a MLFlow registry (puerto 8000)
- **ml-Training Container**: Contenedor que permite la ejecuci√≥n de .py que genera un modelo de ML y lo pasa a producci√≥n en Mlflow de manera autom√°tica
- **Locust Container**: Contenedor en donde se ejecuta Locust para realizar las pruebas de tr√°fico, depende del despliegue del contenedor de FastAPI



## Configuraci√≥n de Infraestructura

### ¬øPor qu√© esta configuraci√≥n?

**Problema original:**
- Se requiere saber cual es la m√≠nima capacidad necesaria para poder soportar 10000 usuarios haciendo request, de igual manera evaluar c√≥mo la generaci√≥n de r√©plicas puede optimizar el proceso, al ser el foco del taller, se busc√≥ automatizar por completo el consumo del modelo para enfocarse en la optimizaci√≥n de request. Adicional, se busca consumir una imagen desde Dockerhub para el despliegue de la API

**Soluci√≥n implementada:**
- Se construy√≥ la imagen del FastAPI y se subio a Dockerhub, una vez en Dockerhub se ajust√≥ el docker compose para consumir esa imagen directamente y desplegar el servicio.Adicionalmente, se realizaron m√∫ltiples experimentos reduciendo la capacidad de los recursos que puede tomar el contenedor de FastAPI, posteriormente se generaron r√©plicas para evaluar el desempe√±o de la API.

### Componentes de Configuraci√≥n

```bash
# Variables de entorno clave para la conexi√≥n de mlflow y fastapi
- MLFLOW_TRACKING_URI

**Funci√≥n:** realiza la conexi√≥n entre FastAPI y Mlflow.
```


### Docker Compose maneja autom√°ticamente:
- Creaci√≥n de redes internas
- Montaje de vol√∫menes persistentes
- Orden de dependencias entre servicios
- Variables de entorno para cada container


## Conexiones Configuradas

### Conexi√≥n de FastAPI con Mlflow

```yaml
# Tracking server de MLflow
        mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
```
* Genera la conexi√≥n directa entre ambos servicios para poder generar la inferencia desde fastAPI tomando los modelos que vayamos desplegando en producci√≥n desde Mlflow
### Conexi√≥n de Locust a FastAPI

```yaml
# environment:
      - LOCUST_HOST=http://10.43.100.98:8000
```
## Flujo del Pipeline

### Secuencia de Ejecuci√≥n:

1. docker compose up
2. Servicios iniciando (Minio+ Postgres + MlFlow + Fastapi + Python Env)
3. docker compose docker-compose-locust up
4. Servicio de Locust iniciado.
5. Definir par√°metros de pruebas de estres 
6. Realizar pruebas de estres reduciendo capacidad hasta llegar al m√≠nimo posible 


## Explicaci√≥n train_model-py (ejecucion.py)

Este script tiene todo el flujo correspondiente a la ingesta de informaci√≥n, entrenamiento, experimentos y paso a producci√≥n del modelo

1. **Preparaci√≥n de la base de datos**
   - Crea el Df de palmerpenguins

2. **Carga y limpieza de datos**
   - Limpia y transforma los datos (One-Hot Encoding, manejo de NaN) 

3. **Entrenamiento del modelo**
   - Guarda todos los logs del experimento en 

4. **Paso a producci√≥n del modelo seleccionado**
   - Se ejecuta un comando para que Mlflow ponga el modelo seleccionado en producci√≥n y pueda ser consumido por la API


**Configuraci√≥n FastAPI**

Framework: FastAPI v3.0.0
Prop√≥sito: Predecir especies de ping√ºinos (Adelie, Chinstrap, Gentoo)
Optimizado para baja latencia y alta concurrencia

**Modelo de datos (PenguinFeatures)**
Recibe 10 caracter√≠sticas del ping√ºino:

Medidas f√≠sicas: longitud/profundidad del pico, longitud de aleta, masa corporal
Variables categ√≥ricas: a√±o, isla (Biscoe/Dream/Torgersen), sexo (female/male)
Todas con validaci√≥n (valores >= 0)

**Carga del modelo (startup)**

Conecta con MLflow para descargar el modelo
Usa MinIO como storage backend (S3-compatible)
Carga el modelo reg_logistica en estado Production
Cachea el modelo en memoria para predicciones instant√°neas
Ejecuta un test de predicci√≥n inicial para validar funcionamiento

**Endpoints**

POST /predict: Recibe caracter√≠sticas, retorna especie predicha con latencia <10ms
GET /health: Estado del servicio y modelo
GET /model-info: Informaci√≥n t√©cnica del modelo cacheado

**Ventaja clave**:
El modelo se carga una sola vez en memoria al iniciar, eliminando el overhead de MLflow en cada predicci√≥n. Esto permite latencias ultra-bajas (<10ms) ideal para producci√≥n de alto tr√°fico.


```


**Resultado final:**  
Se obtiene un modelo de clasificaci√≥n entrenado y validado autom√°ticamente, listo para ser consumido desde FastAPI.


## Instrucciones de Ejecuci√≥n

### Preparaci√≥n Inicial

```bash
# Clonar el repositorio
git clone (https://github.com/DAVID316CORDOVA/MLOps_Taller5)
cd MLOps_Taller5

# Limpiar entorno previo (si existe)
docker compose down -v
docker system prune -f

```

### Ejecuci√≥n 

```bash
# Despu√©s de la preparaci√≥n inicial, simplemente:
docker compose up
```
```bash
#Levantar el servicio de Locust
docker compose docker-compose-locust up
```


**Qu√© sucede**
- Se crean todos los contenedores necesarios
- Se entrena el modelo, se carga y pasa a producci√≥n en MLflow de manera autom√°tica
- La API consume los modelos para hacer la inferencia
- Se carga el Locust para poder realizar las pruebas de estr√©s



## Acceso a Servicios

| Servicio | URL | Credenciales | Descripci√≥n |
|----------|-----|--------------|-------------|
| **Mlflow Web** | http://localhost:5005 | admin/admin | Dashboard del pipeline |
| **FastAPI Docs** | http://localhost:8000/docs | - | API de predicciones |
| **Postgres** | http://localhost:5432 | mlflow_user:mlflow_password | Postgres|
| **Minio** | http://localhost:9000 | admin:supersecret | |
| **Locust** | http://localhost:8089 |  | App de pruebas |


## Ejecuci√≥n del Proyecto

### 1. Despliegue de servicios necesarios (MLflow, FastAPI, Locust)
![Inicio del sistema](./imagenes/dockerhub.jpg)
![Inicio del sistema](./imagenes/mlflow.png)
![Inicio del sistema](./imagenes/locust_1000.jpg)

### 2.Evaluaci√≥n de Recursos Disponibles para los Experimentos
**Resultados:**

![Memoria RAM](./imagenes/free-h.PNG)
![CPU Info](./imagenes/lscpu.PNG)

**Resumen:**
- Memoria RAM: 15 GiB  
- Swap: 4 GiB  
- CPU: 4 n√∫cleos Intel Xeon (2.40 GHz)  
- Sistema operativo: Rocky Linux





### 2. Primera prueba con todos los recursos disponibles
![Inicio del sistema](./imagenes/Recursos%20completos.jpg)

### 3. 1ra disminuci√≥n de recursos (2cpu 4ram)
![Inicio del sistema](./imagenes/2cpu_4ram.png)

## 4. 2da disminuci√≥n de recursos (1.5cpu 3ram)
![Inicio del sistema](./imagenes/1_5_de_cpu_3_ram.png)

## 5. 3ra disminuci√≥n de recursos (1cpu 2ram) 
![Inicio del sistema](./imagenes/1_cpu_2_ram.png)


## 6. 4ta disminuci√≥n de recursos(0.75cpu 1ram)
![Inicio del sistema](./imagenes/0_75_cpu_4_ram.png)

## 7. 5ta disminuci√≥n de recursos(0.75cpu 0.256ram) - Fallo
![Inicio del sistema](./imagenes/0_5_cpu_256mb_ram.png)


## Resumen resultados
| Configuraci√≥n     | RPS M√°ximo | RPS Promedio | Tiempo Resp. Avg (ms) | Tiempo Resp. P95 (ms) | Usuarios M√°x | Fallas/s | Estado        |
|------------------|------------|--------------|----------------------|----------------------|--------------|----------|---------------|
| 0.5CPU - 0.256RAM | ~350       | ~100         | ~145,000             | ~220,000             | 10000        | 1,200+   | ‚ùå Colaps√≥    |
| 0.5 CPU - 1RAM    | ~280       | ~240         | ~26,000              | ~37,000              | 10000        | 0        | ‚úÖ Estable    |
| 0.75CPU - 4RAM    | ~420       | ~360         | ~17,000              | ~24,000              | 10000        | 0        | ‚úÖ Estable    |
| 1.5 CPU - 3RAM    | ~490       | ~420         | ~16,000              | ~22,000              | 10000        | 0        | ‚úÖ Estable    |
| 1CPU - 2RAM       | ~470       | ~390         | ~20,000              | ~23,000              | 10000        | 0        | ‚úÖ Estable    |
| 2CPU - 4RAM       | ~450       | ~370         | ~18,000              | ~23,000              | 10000        | 0        | ‚úÖ Estable    |
| 4CPU - 16RAM      | ~370       | ~330         | ~8,500               | ~18,000              | 10000        | 0        | ‚úÖ √ìptimo     |




### funciones notebook - L√≥gica del Pipeline

```python

import numpy as np 
import mlflow
import mlflow.sklearn
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import palmerpenguins as pp
from sklearn.linear_model import LogisticRegression
from mlflow.tracking import MlflowClient
import os
import time

# Esperar un poco para asegurar que MLflow est√© completamente listo
time.sleep(10)

# ============================================================
# 1. Cargar y preparar los datos
# ============================================================
print("Cargando datos de penguins...")
df = pp.load_penguins()
df.dropna(inplace=True)

# Codificar variables categ√≥ricas
categorical_cols = ['sex', 'island']
encoder = OneHotEncoder(handle_unknown='ignore')
x = df.drop(columns=['species'])
y = df['species']
x_encoded = encoder.fit_transform(x[categorical_cols])
X_numeric = x.drop(columns=categorical_cols)
X_final = np.hstack((X_numeric.values, x_encoded.toarray()))

# Codificaci√≥n simple con pandas (opcional, m√°s legible)
df_encoded = pd.get_dummies(df, columns=['island', 'sex'])
bool_cols = df_encoded.select_dtypes(include='bool').columns
df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)
df_encoded['species'] = df_encoded['species'].apply(lambda x:
                        1 if x == 'Adelie' else
                        2 if x == 'Chinstrap' else
                        3 if x == 'Gentoo' else None)

# ============================================================
# 2. Configurar MLflow
# ============================================================
mlflow_uri = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")
print(f" Conectando a MLflow en {mlflow_uri}")
mlflow.set_tracking_uri(mlflow_uri)
mlflow.set_experiment("experimento")

# ============================================================
# 3. Entrenar el modelo
# ============================================================
print("ü§ñ Entrenando modelo...")
df = df_encoded
X = df.drop("species", axis=1)
y = df["species"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

model = LogisticRegression(max_iter=5000)
model.fit(X_train, y_train)

# ============================================================
# 4. Loguear y registrar el modelo en MLflow
# ============================================================
print("Registrando modelo en MLflow...")
with mlflow.start_run(run_name="logistic_regression_run") as run:
    # Registrar m√©tricas
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", acc)

    # Log del modelo
    mlflow.sklearn.log_model(
        sk_model=model,
        artifact_path="model",
        input_example=X_test.head(1),  # ejemplo de entrada
        registered_model_name="reg_logistica"  # nombre del modelo en el registry
    )

    print(f"Modelo logueado con accuracy = {acc:.4f}")

# ============================================================
# 5. Promover el modelo a Producci√≥n
# ============================================================
print("Promoviendo modelo a Production...")
client = MlflowClient()

model_name = "reg_logistica"
model_version = 1  # la primera versi√≥n que acabas de crear

try:
    client.transition_model_version_stage(
        name=model_name,
        version=model_version,
        stage="Production",
        archive_existing_versions=True  # mueve versiones anteriores a Archived
    )
    print(f" Modelo {model_name} v{model_version} promovido a Production")
except Exception as e:
    print(f" Error al promover modelo: {e}")

print("üéâ ¬°Proceso completado exitosamente!")

```
**Fast API Optimizado**
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import pandas as pd
import logging
import os
import mlflow
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Penguins Species Prediction API",
    description="API optimizada para alta concurrencia",
    version="3.0.0"
)

# Variables globales
model = None
model_loaded_at = None
species_mapping = {1: "Adelie", 2: "Chinstrap", 3: "Gentoo"}

class PenguinFeatures(BaseModel):
    bill_length_mm: float = Field(..., example=39.1, ge=0)
    bill_depth_mm: float = Field(..., example=18.7, ge=0)
    flipper_length_mm: float = Field(..., example=181.0, ge=0)
    body_mass_g: float = Field(..., example=3750.0, ge=0)
    year: int = Field(..., example=2007, ge=0)
    island_Biscoe: int = Field(0, example=0, ge=0)
    island_Dream: int = Field(0, example=0, ge=0)  
    island_Torgersen: int = Field(1, example=1, ge=0)
    sex_female: int = Field(0, example=0, ge=0)
    sex_male: int = Field(1, example=1, ge=0)

@app.on_event("startup")
async def load_model():
    global model, model_loaded_at
    try:
        logger.info("Iniciando carga del modelo...")
        
        # Configuraci√≥n MLflow
        os.environ["MLFLOW_S3_ENDPOINT_URL"] = os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://minio:9000")
        os.environ["AWS_ACCESS_KEY_ID"] = os.getenv("AWS_ACCESS_KEY_ID", "admin")
        os.environ["AWS_SECRET_ACCESS_KEY"] = os.getenv("AWS_SECRET_ACCESS_KEY", "supersecret")
        mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5008"))
        
        MODEL_NAME = "reg_logistica"
        MODEL_STAGE = "Production"
        model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
        
        logger.info(f"Descargando modelo desde: {model_uri}")
        model = mlflow.pyfunc.load_model(model_uri)
        model_loaded_at = time.time()
        
        logger.info(f"Modelo cargado en memoria. Tipo: {type(model).__name__}")
        
        # Test de predicci√≥n
        test_data = pd.DataFrame([{
            "bill_length_mm": 39.1,
            "bill_depth_mm": 18.7,
            "flipper_length_mm": 181.0,
            "body_mass_g": 3750.0,
            "year": 2007,
            "island_Biscoe": 0,
            "island_Dream": 0,
            "island_Torgersen": 1,
            "sex_female": 0,
            "sex_male": 1
        }])
        test_pred = model.predict(test_data)[0]
        logger.info(f"Test de predicci√≥n: {test_pred} ({species_mapping.get(int(test_pred), 'Unknown')})")
        
    except Exception as e:
        logger.error(f" Error cargando modelo: {e}")
        raise e

@app.post("/predict")
def predict(features: PenguinFeatures):
    if model is None:
        raise HTTPException(status_code=503, detail="Modelo no disponible")
    
    try:
        # Convertir a DataFrame (compatible con pyfunc)
        feature_df = pd.DataFrame([features.dict()])
        prediction = model.predict(feature_df)[0]
        
        return {
            "species_id": int(prediction),
            "species_name": species_mapping.get(int(prediction), "Unknown"),
            "latency_ms": "<10ms",
            "model_source": "In-Memory Cache"
        }
    except Exception as e:
        logger.error(f" Error en predicci√≥n: {e}")
        raise HTTPException(status_code=400, detail=f"Error: {str(e)}")

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "model_loaded_at": model_loaded_at
    }

@app.get("/model-info")
def model_info():
    if model is None:
        raise HTTPException(status_code=503, detail="Modelo no disponible")
    return {
        "model_type": str(type(model).__name__),
        "model_loaded": True,
        "model_in_memory": True,
        "description": "Modelo cacheado en memoria - predicciones instant√°neas sin overhead de MLflow"
    }
```



## Conclusiones

Conclusiones

La implementaci√≥n realizada demuestra c√≥mo integrar de manera efectiva distintos servicios (MySQL, Postgres, MinIO, Jupyter, FastAPI y MLflow) en un entorno contenerizado con Docker Compose, logrando un flujo completo de MLOps.

Se logr√≥ establecer un pipeline automatizado que cubre todas las etapas: ingesta de datos, limpieza, entrenamiento, experimentaci√≥n, registro de modelos, despliegue y consumo en producci√≥n.

El uso de MLflow con backend en Postgres y artefactos en MinIO garantiza trazabilidad, persistencia y escalabilidad en comparaci√≥n con la configuraci√≥n por defecto en SQLite.

La conexi√≥n entre FastAPI y MLflow permite consumir modelos directamente desde el registro, habilitando predicciones en tiempo real y demostrando un caso de uso cercano a un escenario de producci√≥n.

El notebook desarrollado constituye un ejemplo reproducible de todo el flujo de trabajo, desde la preparaci√≥n de la base de datos hasta la promoci√≥n de un modelo en producci√≥n, lo que refuerza la importancia de la automatizaci√≥n y reproducibilidad en proyectos de ciencia de datos.

En general, este proyecto evidencia la viabilidad y relevancia de aplicar pr√°cticas de MLOps en entornos acad√©micos y profesionales, sirviendo como base para escalar a soluciones m√°s complejas en el futuro.

---

**Desarrollado por:**
- Sebastian Rodr√≠guez  
- David C√≥rdova

**Proyecto:** MLOps Taller 5 - Locust
**Fecha:** Octubre 2025
